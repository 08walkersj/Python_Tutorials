{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance\n",
    "In this notebook we show examples of how to do feature importance\n",
    "\n",
    "### Matplotlib backend\n",
    "pick a back end that can display the figure and allow interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.widgets import Slider\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier\n",
    "In this example we are looking at binary data (i.e. True or False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification # used to make example data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y= make_classification(n_samples=100, n_features=5, \n",
    "                               n_informative=2, n_redundant=0, \n",
    "                               n_classes=2, random_state=42)\n",
    "# make one feature that is a perfect match to the target variable\n",
    "X[y==0, 0]=-1\n",
    "X[y==1, 0]=1\n",
    "# make a second feature is not as good a match\n",
    "X[y==0, 1]=2\n",
    "X[y==1, 1]=-2\n",
    "ind= np.where(y==0)[0]\n",
    "ind=ind[:int(len(ind)/2)]\n",
    "X[ind, 1]=np.random.uniform(-5, 5, size=len(ind))\n",
    "ind= np.where(y==1)[0]\n",
    "ind=ind[int(len(ind)/2):]\n",
    "X[ind, 1]=np.random.uniform(-5, 5, size=len(ind))\n",
    "\n",
    "# the remaining features are purely random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Bar Chart Feature Importance\n",
    "We add a slider that adds noise to all features to show what happens to the feature importance as it is more poorly correlated with the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate data, train the model, and plot feature importances\n",
    "def plot_feature_importance(noise_level, ax, bar=False):\n",
    "    X_= X.copy()\n",
    "    X_= X_+ np.random.binomial(n=noise_level, p=0.5, size=X.shape)\n",
    "    # Convert to DataFrame for convenience\n",
    "    X_df = pd.DataFrame(X_, columns=[f'Feature_{i}' for i in range(len(X.T))])    \n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_df, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Initialize and train RandomForestClassifier\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Get feature importances\n",
    "    feature_importances_clf = pd.Series(clf.feature_importances_, index=X_df.columns).sort_values(ascending=False)\n",
    "    \n",
    "    ax.figure.suptitle(f'Substorm Onsets \\n Feature Importances for RandomForestClassifier (Noise Level = {noise_level})')\n",
    "    \n",
    "    if bar:\n",
    "        # Update existing bars with new heights\n",
    "        for b, val in zip(bar, feature_importances_clf.values):\n",
    "            b.set_height(val)\n",
    "        plt.draw()\n",
    "    else:\n",
    "        # Plot the feature importances using a bar chart\n",
    "        return ax.bar(range(len(feature_importances_clf)), height=feature_importances_clf, color='salmon')\n",
    "\n",
    "\n",
    "# Create the initial figure and axes\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_axes([0.2, 0.1, .7, 0.8])\n",
    "ax.set_ylabel('Importance')\n",
    "ax.set_xlabel('Features')\n",
    "ax.set_xticks(list(range(5)))\n",
    "ax.set_xticklabels(['AL Index', 'IMF Bz', 'Solar Zenith Angle', 'Ireland Population', 'rainfall'])\n",
    "# Initial plot with noise level 0\n",
    "bar = plot_feature_importance(0, ax=ax)\n",
    "\n",
    "# Create a slider for controlling noise level\n",
    "axamp = fig.add_axes([0.1, 0.1, 0.0225, 0.8])\n",
    "noise_slider = Slider(ax=axamp, valmin=0, valmax=20, label='Noise Level', valstep=1, valinit=0, orientation='vertical')\n",
    "\n",
    "# Update function for slider\n",
    "def update(noise_level):\n",
    "    plot_feature_importance(noise_level, ax=ax, bar=bar)\n",
    "    fig.canvas.draw_idle()\n",
    "\n",
    "# Call the update function when slider value changes\n",
    "noise_slider.on_changed(update)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Confusion Matrix\n",
    "Here we use a confusion matrix to show how good predictions are. We use the train test split creating the model on the train set and testing the accuracy using the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Generate example data\n",
    "X, y = make_classification(n_samples=500, n_features=5, \n",
    "                           n_informative=2, n_redundant=0, \n",
    "                           n_classes=2, random_state=42)\n",
    "\n",
    "# Make one feature that is a perfect match to the target variable\n",
    "X[y == 0, 0] = -1\n",
    "X[y == 1, 0] = 1\n",
    "\n",
    "# Make a second feature that is not as good a match\n",
    "X[y == 0, 1] = 2\n",
    "X[y == 1, 1] = -2\n",
    "\n",
    "# Add noise to some samples in the second feature\n",
    "ind = np.where(y == 0)[0]\n",
    "ind = ind[:int(len(ind) / 2)]\n",
    "X[ind, 1] = np.random.uniform(-5, 5, size=len(ind))\n",
    "ind = np.where(y == 1)[0]\n",
    "ind = ind[int(len(ind) / 2):]\n",
    "X[ind, 1] = np.random.uniform(-5, 5, size=len(ind))\n",
    "\n",
    "# Function to generate data, train the model, and calculate predictions\n",
    "def get_vals(noise_level):\n",
    "    X_ = X.copy()\n",
    "    # Add noise to the features\n",
    "    X_ = X_ + np.random.binomial(n=noise_level, p=0.5, size=X_.shape)\n",
    "    \n",
    "    # Convert to DataFrame for convenience\n",
    "    X_df = pd.DataFrame(X_, columns=[f'Feature_{i}' for i in range(len(X_.T))])\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_df, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Initialize and train RandomForestClassifier\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "    return y_test, y_pred\n",
    "\n",
    "# Function to plot the confusion matrix\n",
    "def plot_confusion_matrix(noise, ax, overwrite_image=None, colorbar=False):\n",
    "    y_test, y_pred = get_vals(noise)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    ax.clear()  # Clear the previous plot\n",
    "\n",
    "    # Create a confusion matrix display\n",
    "    cmd = ConfusionMatrixDisplay(cm, display_labels=[\"No Onset\", \"Onset\"])\n",
    "    ax.set_title(f'Confusion Matrix (Noise Level = {noise})')\n",
    "    return cmd.plot(ax=ax, cmap='Blues', values_format='d', im_kw={'vmin':0, 'vmax':100}, colorbar=colorbar)\n",
    "\n",
    "\n",
    "# Create the initial figure and axes\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "# Initial plot with noise level 0\n",
    "im= plot_confusion_matrix(0, ax=ax, colorbar=True)\n",
    "\n",
    "# Create a slider for controlling noise level\n",
    "axamp = fig.add_axes([0.1, 0.1, 0.0225, 0.8])\n",
    "noise_slider = Slider(ax=axamp, valmin=0, valmax=20, label='Noise Level', valstep=1, valinit=0, orientation='vertical')\n",
    "\n",
    "# Update function for slider\n",
    "def update(noise_level):\n",
    "    # Overwrite the existing confusion matrix with the new data for the selected noise level\n",
    "    plot_confusion_matrix(noise_level, ax=ax)\n",
    "    plt.draw()\n",
    "    fig.canvas.draw_idle()\n",
    "\n",
    "# Call the update function when slider value changes\n",
    "noise_slider.on_changed(update)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor\n",
    "In this example we are looking at continuous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "n_samples = 100\n",
    "y_regression = np.random.randn(n_samples)\n",
    "\n",
    "# Function to generate the data, fit the model, and plot feature importances\n",
    "def plot_model_accuracy(noise_level, ax, bar=False):\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Number of features\n",
    "    n_features = 5\n",
    "\n",
    "    # Generate features\n",
    "    # High importance feature - strong linear relationship with target\n",
    "    # Low importance feature - random noise\n",
    "    low_importance_feature = np.random.randn(n_samples)\n",
    "\n",
    "    # Generate remaining features (random noise)\n",
    "    other_features = np.random.randn(n_samples, n_features - 2)\n",
    "\n",
    "    # Target variable - strong relationship with high_importance_feature + some noise\n",
    "    high_importance_feature = 5 * y_regression + np.random.randn(n_samples) * 0.5  # High importance feature dominates\n",
    "\n",
    "    # Add noise based on the noise_level slider value\n",
    "    high_importance_feature += np.random.normal(0, noise_level, size=high_importance_feature.shape)\n",
    "\n",
    "    # Combine features into a DataFrame\n",
    "    X_regression = pd.DataFrame(np.column_stack([high_importance_feature, low_importance_feature, other_features]),\n",
    "                                columns=['High_Importance_Feature', 'Low_Importance_Feature'] + [f'Feature_{i}' for i in range(n_features - 2)])\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_regression, y_regression, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Initialize and train RandomForestRegressor\n",
    "    reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    reg.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "    # Get feature importances\n",
    "    feature_importances_reg = pd.Series(reg.feature_importances_, index=X_regression.columns).sort_values(ascending=False)\n",
    "    ax.figure.suptitle(f'Feature Importances for RandomForestRegressor (Noise Level = {noise_level})')\n",
    "\n",
    "    if bar:\n",
    "        for b, val in zip(bar, feature_importances_reg.values):\n",
    "            b.set_height(val)\n",
    "        plt.draw()\n",
    "    else:\n",
    "        # Plot the feature importances using a bar chart\n",
    "        return ax.bar(range(len(feature_importances_reg)), height=feature_importances_reg, color='salmon')\n",
    "\n",
    "\n",
    "fig=plt.figure()\n",
    "ax=fig.add_axes([0.2, 0.1, .5, 0.8])\n",
    "ax.set_ylabel('Importance')\n",
    "ax.set_xlabel('Features')\n",
    "ax.set_xticks(list(range(5)))\n",
    "ax.set_xticklabels(['AL Index', 'IMF Bz', 'Solar Zenith Angle', 'Ireland Population', 'rainfall'])\n",
    "bar= plot_feature_importance(0, ax=ax)\n",
    "\n",
    "\n",
    "axamp = fig.add_axes([0.1, 0.1, 0.0225, 0.8])\n",
    "noise_slider = Slider(ax=axamp, valmin=0.0, valmax=20.0, label='Noise Level', valinit=0, orientation='vertical')\n",
    "def update(noise_level):\n",
    "    plot_feature_importance(noise_level, ax=ax, bar=bar)\n",
    "    fig.canvas.draw_idle()\n",
    "noise_slider.on_changed(update)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Accuracy\n",
    "Here we test the model accuracy with a line plot by plotting the predictions against the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "n_samples = 100\n",
    "y_regression = np.random.randn(n_samples)\n",
    "\n",
    "# Function to generate the data, fit the model, and plot feature importances\n",
    "def plot_model_accuracy(noise_level, ax, scatter=False):\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Number of features\n",
    "    n_features = 5\n",
    "\n",
    "    # Generate features\n",
    "    # High importance feature - strong linear relationship with target\n",
    "    # Low importance feature - random noise\n",
    "    low_importance_feature = np.random.randn(n_samples)\n",
    "\n",
    "    # Generate remaining features (random noise)\n",
    "    other_features = np.random.randn(n_samples, n_features - 2)\n",
    "\n",
    "    # Target variable - strong relationship with high_importance_feature + some noise\n",
    "    high_importance_feature = 5 * y_regression + np.random.randn(n_samples) * 0.5  # High importance feature dominates\n",
    "\n",
    "    # Add noise based on the noise_level slider value\n",
    "    high_importance_feature += np.random.normal(0, noise_level, size=high_importance_feature.shape)\n",
    "\n",
    "    # Combine features into a DataFrame\n",
    "    X_regression = pd.DataFrame(np.column_stack([high_importance_feature, low_importance_feature, other_features]),\n",
    "                                columns=['High_Importance_Feature', 'Low_Importance_Feature'] + [f'Feature_{i}' for i in range(n_features - 2)])\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_regression, y_regression, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Initialize and train RandomForestRegressor\n",
    "    reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    reg.fit(X_train_reg, y_train_reg)\n",
    "    \n",
    "    y_pred= reg.predict(X_test_reg)\n",
    "\n",
    "    ax.figure.suptitle(f'Model Fit for RandomForestRegressor (Noise Level = {noise_level})')\n",
    "\n",
    "    if scatter:\n",
    "        scatter.set_offsets(np.array([y_test_reg, y_pred]).T)\n",
    "        plt.draw()\n",
    "    else:\n",
    "        return ax.scatter(y_test_reg, y_pred, color='salmon')\n",
    "\n",
    "\n",
    "fig=plt.figure()\n",
    "ax=fig.add_axes([0.2, 0.1, .5, 0.8])\n",
    "ax.set_ylabel('Model')\n",
    "ax.set_xlabel('Truth')\n",
    "ax.set_xlim(-10, 10)\n",
    "ax.set_ylim(-10, 10)\n",
    "ax.set_aspect('equal')\n",
    "scatter= plot_model_accuracy(0, ax=ax)\n",
    "\n",
    "\n",
    "axamp = fig.add_axes([0.1, 0.1, 0.0225, 0.8])\n",
    "noise_slider = Slider(ax=axamp, valmin=0.0, valmax=20.0, label='Noise Level', valinit=0, orientation='vertical')\n",
    "def update(noise_level):\n",
    "    plot_model_accuracy(noise_level, ax=ax, scatter=scatter)\n",
    "    fig.canvas.draw_idle()\n",
    "noise_slider.on_changed(update)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
